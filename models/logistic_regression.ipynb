{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Regression with Tf-Idf Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read data as pandas dataframe\n",
    "data = pd.read_csv('../raw_data/fulltrain.csv', names=['label', 'text'])\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# found out that fulltrain.csv has 202 duplicate rows => remove them before proceeding\n",
    "data = data.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "Counter(data['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create tf-idf matrix\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer(ngram_range=(1, 1), max_features=9500, max_df=0.6) # HYPERPARAMETERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "full_train_data = data.copy()\n",
    "train_data, eval_data = train_test_split(full_train_data, test_size=0.2, random_state=42)\n",
    "print(train_data.shape)\n",
    "print(eval_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = vectorizer.fit_transform(train_data['text'])\n",
    "X_eval = vectorizer.transform(eval_data['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LABEL = 'label'\n",
    "TEXT = 'text'\n",
    "\n",
    "train_label = train_data[LABEL]\n",
    "eval_label = eval_data[LABEL]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# sm = SMOTE(random_state=42)\n",
    "# X_train_balanced, train_label_balanced = sm.fit_resample(X_train, train_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"original training data:\", Counter(full_train_data[LABEL]))\n",
    "# print(\"balanced training data:\", Counter(train_label_balanced))\n",
    "print(\"evaluation data:\", Counter(eval_label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LogisticRegression(max_iter=1000, C=0.15, class_weight='balanced', penalty=\"l2\")\n",
    "model.fit(X_train, train_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print evaluation metrics\n",
    "print('Accuracy: ', accuracy_score(eval_label, y_pred))\n",
    "print('F1: ', f1_score(eval_label, y_pred, average='macro'))\n",
    "print('Precision: ', precision_score(eval_label, y_pred, average='macro'))\n",
    "print('Recall: ', recall_score(eval_label, y_pred, average='macro'))\n",
    "print(classification_report(eval_label, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This makes me feel that the test data is somehow fundamentally different from the evaluation (and training) data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sanity check for test data\n",
    "test_data = pd.read_csv('../raw_data/balancedtest.csv', names=['label', 'text'])\n",
    "test_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = vectorizer.transform(test_data['text'])\n",
    "test_label = test_data[LABEL]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pred = model.predict(X_test)\n",
    "test_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print evaluation metrics for test data\n",
    "print('Accuracy: ', accuracy_score(test_label, test_pred))\n",
    "print('F1: ', f1_score(test_label, test_pred, average='macro'))\n",
    "print('Precision: ', precision_score(test_label, test_pred, average='macro'))\n",
    "print('Recall: ', recall_score(test_label, test_pred, average='macro'))\n",
    "print(classification_report(test_label, test_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Dive into Logistic Regression\n",
    "\n",
    "The goal of this deep dive is to figure out WHY the model performs so much worse on the test data compared to the evaluation data.\n",
    "\n",
    "We aim to analyze:\n",
    "\n",
    "- Which features does the model think are important?\n",
    "- Does it give too much importance to named entities?\n",
    "- We will carefully look at which sentences does the model misclassify, and hope to understand why.\n",
    "- We will also look at the confusion matrix to see if the model is misclassifying a particular class more than others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocabulary = vectorizer.vocabulary_ # word: index\n",
    "inverse_vocabulary = {v: k for k, v in vocabulary.items()} # index: word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each sentence (document), we want to know which words the model is paying more attention to. We want to find the coefficients of the model for each word in the sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coefficients = model.coef_[0]\n",
    "word_coefficients = [(inverse_vocabulary[i], coefficients[i]) for i in range(len(coefficients))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_word_coefficients = sorted(word_coefficients, key=lambda x: abs(x[1]), reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for word, coef in sorted_word_coefficients[:10]:\n",
    "    print(word, coef)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "abuse_sources = [\n",
    "    'Tuesday is a good day',\n",
    "    'Wednesday is a good day',\n",
    "    'Thursday is a good day',\n",
    "    'Friday is a good day',\n",
    "]\n",
    "abuse_sources = model.predict(vectorizer.transform(abuse_sources))\n",
    "abuse_sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find all sentences with the word \"tuesday\" and count their labels\n",
    "tuesday_sentences = full_train_data[full_train_data['text'].str.contains('tuesday', case=False)]\n",
    "tuesday_sentences['label'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To us, it seems very strange that the model treats days so differently - it literally changes the class of a sentence depending on which day you're talking about. This is clearly not a good strategy. It's likely that \"Tuesday\" occurred most commonly in satirical sentences, and the model learnt to be cautious of sentences with the word \"Tuesday\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trump vs biden, WOW this is a big deal!\n",
    "print(word_coefficients[vocabulary['trump']])\n",
    "print(word_coefficients[vocabulary['biden']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trump_biden_sentences = [\n",
    "    'Trump is the best president.',\n",
    "    'Biden is the best president.',\n",
    "    'Trump was a president.',\n",
    "]\n",
    "trump_biden_predictions = model.predict(vectorizer.transform(trump_biden_sentences))\n",
    "print(trump_biden_predictions)\n",
    "trump_biden_probabilities = model.predict_proba(vectorizer.transform(trump_biden_sentences))\n",
    "print(trump_biden_probabilities) # the model seems to be quite confident (>90%) when classifying a sentence with \"Trump\" to be a hoax."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above result can be unsettling. The only difference between the two sentences is that I've replaced Trump with Biden, and the model proceeds to change it's classification from satire to hoax. At least we can take comfort knowing that it doesn't classify it as reliable :O\n",
    "\n",
    "Moreover, the model seems to be quite confident (>90%) when classifying a sentence with \"Trump\" to be a hoax."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some more pairs of weird words\n",
    "print(word_coefficients[vocabulary['washington']])\n",
    "print(word_coefficients[vocabulary['moscow']])\n",
    "print(word_coefficients[vocabulary['china']])\n",
    "\n",
    "city_sentences = [\n",
    "    'Washington is a good place to work',\n",
    "    'Moscow is a good place to work',\n",
    "    'China is a good place to work',\n",
    "]\n",
    "vectorizer.transform(city_sentences)\n",
    "city_predictions = model.predict(vectorizer.transform(city_sentences))\n",
    "print(city_predictions)\n",
    "city_probabilities = model.predict_proba(vectorizer.transform(city_sentences))\n",
    "print(city_probabilities)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above example CLEARLY shows that the model is biased towards washington (possibly because the model was trained on a dataset where washington was a common word in reliable news articles). This is a clear example of bias in the model.\n",
    "\n",
    "Of course, it doesn't mean that any sentence involving Washington automatically becomes more reliable than sentences involving China or Moscow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we also want to figure out what the model is getting wrong, i.e., which class does it get most confused by\n",
    "# for this, we can use a confusion matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# relabel the classes to start from 1 instead of 0\n",
    "conf_matrix = confusion_matrix(np.array(test_label) + 1, np.array(test_pred) + 1)\n",
    "sns.heatmap(conf_matrix, annot=True, fmt='d', xticklabels=[1, 2, 3, 4], yticklabels=[1, 2, 3, 4])\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can make the following observations from the above model:\n",
    "- Even though the overall F1 score of the model is not very high (~0.70), it is able to classify the majority of the reliable news articles, as being reliable. This means we have a low false positive rate (it doesn't \"catch\" many reliable news articles as being unreliable)\n",
    "- There are 2 main issues that the model faces: \n",
    "  - It classifies many hoax articles as being propaganda, but surprisingly, it doesn't classify many propaganda articles as being hoax.\n",
    "  - It also classifies many propaganda articles as being reliable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Honestly the second kind of issue is more worrisome. Because it fails to catch propaganda articles as being unreliable. This is a more dangerous issue. More generally speaking, propaganda articles tend to use authoritative language, and are more likely to be longer, making them sound more \"convincing\". This is also why humans find it difficult to distinguish between propaganda and reliable news articles.\n",
    "\n",
    "It's not just humans though. It's been shown that the youtube recommendation algorithm also ranks more authoritative videos higher, even if they are spreading misinformation. This is a very difficult problem to solve, and it's not clear if we can solve it with a simple logistic regression model, or any algorithmic model for that matter.\n",
    "\n",
    "There is no algorithm for truth."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Looking at which categories of sentences are actually misclassified\n",
    "\n",
    "Are the most misclassified sentences from: health? environment? politics? etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the indices of all the test data that were misclassified\n",
    "misclassified_indices = np.where(test_label != test_pred)[0]\n",
    "\n",
    "# out of these, find the ones whose ground truth is 3 (propaganda), but the model predicted 4 (reliable)\n",
    "misclassified_indices_3_4 = [i for i in misclassified_indices if test_label[i] == 3 and test_pred[i] == 4]\n",
    "\n",
    "print(len(misclassified_indices_3_4))\n",
    "# and then print those sentences\n",
    "for i in misclassified_indices_3_4:\n",
    "    print(test_data.iloc[i][TEXT])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reading the sentences above, it's clear that nearly all of the 105 misclassified sentences are from the health industry (they discuss topics such as diets, food, medicine, etc.) and environment industry (they discuss topics such as climate change, pollution, etc.). This is a very interesting observation. It seems that the model is not able to distinguish between reliable and unreliable news articles in these industries. This makes some sense because the majority of the sentences in the dataset are from the politics industry, and so, the model is unable to generalize beyond the politics industry.\n",
    "\n",
    "In fact, the model performs well only on sentences relating to _American_ (or Western) politics and business, not other countries. Again, unsurprising because the dataset is primarily on American politics and business."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vaccine_sentences = [\n",
    "    'Vaccines are useful',\n",
    "    'Vaccines are not useful',\n",
    "]\n",
    "print(word_coefficients[vocabulary['vaccine']])\n",
    "vaccine_predictions = model.predict(vectorizer.transform(vaccine_sentences))\n",
    "print(vaccine_predictions) # both are classified as propaganda\n",
    "vaccine_probabilities = model.predict_proba(vectorizer.transform(vaccine_sentences))\n",
    "vaccine_probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print all training sentences with the word vaccine\n",
    "vaccine_indices = [i for i in range(len(train_data)) if 'vaccine' in train_data.iloc[i][TEXT]]\n",
    "freq_class = Counter(train_data.iloc[vaccine_indices][LABEL])\n",
    "for i in vaccine_indices:\n",
    "    print(train_data.iloc[i][TEXT], train_data.iloc[i][LABEL])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One interesting thing is the model is over 90% confident in its prediction, and is still wrong!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
