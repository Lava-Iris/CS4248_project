{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from os.path import join\n",
    "import pickle as pkl\n",
    "\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load processed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pkl_load(path):\n",
    "    with open(path,'rb') as f:\n",
    "        return pkl.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All processed data are stored under the folder \"processed_data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "parent_directory = os.path.dirname(os.getcwd())\n",
    "processed_data_path = join(parent_directory,\"processed_data\")\n",
    "\n",
    "# load the vocabulary\n",
    "vocab_path = join(processed_data_path,\"remove-stopwords-punct-25000.vocab\")\n",
    "vocab = torch.load(vocab_path)\n",
    "\n",
    "\n",
    "# load training, validation and test sets\n",
    "train_path = join(processed_data_path,\"train.pickle\")\n",
    "val_path = join(processed_data_path,\"val.pickle\")\n",
    "test_path = join(processed_data_path,\"test.pickle\")\n",
    "\n",
    "X_train,y_train = pkl_load(train_path)\n",
    "X_val,y_val = pkl_load(val_path)\n",
    "X_test,y_test = pkl_load(test_path)\n",
    "\n",
    "\n",
    "# load embedding matrix\n",
    "word2vec_path = join(processed_data_path,\"word2vec.pickle\")\n",
    "glove_path = join(processed_data_path,\"glove.pickle\")\n",
    "\n",
    "embed_matrix_word2vec = pkl_load(word2vec_path)\n",
    "embed_matrix_glove = pkl_load(glove_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The X's are nested lists in the form of list[list[int]]. <br>\n",
    "The inner lists contain tokenized texts, with each token represented by its index in the vocabulary. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[11, 52, 1450, 54, 910]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[1][:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['new', 'law', 'proposed', 'that', 'will']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab.lookup_tokens(X_train[1][:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The y's are lists of labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 1, 3]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get pre-trained embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The embedding matrixs are in the shape of (len(vocab),embed_dim). Row i represents the i-th token's pre-trained embedding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(25002, 300)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00, ...,\n",
       "         0.00000000e+00,  0.00000000e+00,  0.00000000e+00],\n",
       "       [-2.25727392e-04, -1.01302849e-03, -1.04770563e-02, ...,\n",
       "        -2.76348446e-02,  6.10916867e-02,  3.80460705e-02],\n",
       "       [ 8.00781250e-02,  1.04980469e-01,  4.98046875e-02, ...,\n",
       "         3.66210938e-03,  4.76074219e-02, -6.88476562e-02],\n",
       "       ...,\n",
       "       [ 3.03955078e-02,  3.32031250e-01,  5.27954102e-03, ...,\n",
       "        -3.18359375e-01,  3.04687500e-01,  1.67968750e-01],\n",
       "       [ 6.87500000e-01, -1.20605469e-01,  1.08642578e-02, ...,\n",
       "        -2.20703125e-01,  2.85156250e-01, -1.57226562e-01],\n",
       "       [-7.71484375e-02,  2.59765625e-01, -8.49609375e-02, ...,\n",
       "        -7.71484375e-02, -1.57226562e-01,  1.22558594e-01]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(embed_matrix_word2vec.shape)\n",
    "embed_matrix_word2vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Row 0 is the embedding vector for \\<PAD>. I use zero vector to represent it since it does not have any meaning.<br>\n",
    "Row 1 is the embedding vector for \\<UNK>. I use the mean of all pre-trained vectors to represent it since it could be any word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embed_matrix_word2vec[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310_gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
