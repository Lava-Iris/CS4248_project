{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pickle as pkl\n",
    "import torch\n",
    "\n",
    "# load the vocabulary\n",
    "vocab_path = \"../processed_data/remove-stopwords-punct-25000.vocab\"\n",
    "\n",
    "vocab = torch.load(vocab_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class embedding:\n",
    "    def __init__(self, pretrained_embeds, vocab):\n",
    "        self.embeds = pretrained_embeds\n",
    "        self.vocab = vocab\n",
    "        self.embed_dim = pretrained_embeds.vector_size\n",
    "        self.vocab_size = len(vocab)\n",
    "        self.unk_embed = self.get_unk_embed()\n",
    "\n",
    "    def get_unk_embed(self):\n",
    "        \"\"\"use the mean of all word embeddings as the embedding for <unk>\"\"\"\n",
    "        embed_sum = np.zeros(self.embed_dim)\n",
    "        for word in self.embeds.key_to_index:\n",
    "            embed_sum += self.embeds[word]\n",
    "        return embed_sum / len(self.embeds.key_to_index)\n",
    "\n",
    "    def get_embed_matrix(self):\n",
    "        \"\"\"return the embedding matrix\"\"\"\n",
    "\n",
    "        # Initialize the embedding matrix\n",
    "        embed_matrix = np.zeros((self.vocab_size,self.embed_dim))\n",
    "\n",
    "        for i, token in enumerate(vocab.get_itos()):\n",
    "            if token == '<PAD>':\n",
    "                # use zero vector as the embedding for <PAD>\n",
    "                continue\n",
    "            elif token == '<UNK>':\n",
    "                embed_matrix[i] = self.unk_embed\n",
    "            elif token in self.embeds.key_to_index:\n",
    "                embed_matrix[i] = self.embeds[token]\n",
    "            else:   \n",
    "                # token is not found in pre-trained embeddings\n",
    "                # use a normalized random vector to represent it\n",
    "                rand_vec = np.random.normal(0,1,size=self.embed_dim)\n",
    "                embed_matrix[i] = rand_vec / rand_vec.max()\n",
    "        return embed_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.downloader\n",
    "\n",
    "word2vec_vectors = gensim.downloader.load('word2vec-google-news-300')\n",
    "glove_vectors = gensim.downloader.load('glove-wiki-gigaword-300')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec = embedding(word2vec_vectors, vocab)\n",
    "word2vec_embeds = word2vec.get_embed_matrix()\n",
    "\n",
    "glove = embedding(glove_vectors, vocab)\n",
    "glove_embeds = glove.get_embed_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the results\n",
    "def pkl_save(path,obj):\n",
    "    with open(path,'wb') as f:\n",
    "        pkl.dump(obj,f)\n",
    "\n",
    "word2vec_name = \"../processed_data/word2vec.pickle\"\n",
    "glove_name = \"../processed_data/glove.pickle\"\n",
    "\n",
    "pkl_save(word2vec_name,word2vec_embeds)\n",
    "pkl_save(glove_name,glove_embeds)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310_gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
